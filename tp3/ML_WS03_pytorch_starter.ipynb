{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5c3f313",
   "metadata": {},
   "source": [
    "# 2CSSID Workshop03. Neural networks' tools (Pytorch)\n",
    "\n",
    "<p style='text-align: right;font-style: italic; color: red;'>Designed by: Mr. Abdelkrime Aries</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e40314",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5823642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.1+cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor, nn, optim\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd29ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pytorch-lightning\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3919a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas     as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a79a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce92c408",
   "metadata": {},
   "source": [
    "## I. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a2fcff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/sat.trn', delimiter=' ', header=None)\n",
    "\n",
    "# Generate column names as 'column1', 'column2', ..., 'columnN'\n",
    "df.columns = [f'column{i+1}' for i in range(df.shape[1])]\n",
    "\n",
    "# Rename the last column to 'output'\n",
    "df.columns = list(df.columns[:-1]) + ['output']\n",
    "\n",
    "x_train = df.iloc[:, :-1].values  # Selecting all columns except the last one\n",
    "y_train = df.iloc[:, -1].values  # Selecting the last column as y_train\n",
    "\n",
    "# Normalize x_train (divide by 255)\n",
    "x_train = x_train / 255.0\n",
    "\n",
    "# Apply LabelBinarizer to y_train\n",
    "label_binarizer = LabelBinarizer()\n",
    "y_train = label_binarizer.fit_transform(y_train)\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train)\n",
    "y_train_tensor = torch.tensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6356a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/sat.tst', delimiter=' ', header=None)\n",
    "\n",
    "# Generate column names as 'column1', 'column2', ..., 'columnN'\n",
    "test_df.columns = [f'column{i+1}' for i in range(test_df.shape[1])]\n",
    "\n",
    "# Rename the last column to 'output'\n",
    "test_df.columns = list(test_df.columns[:-1]) + ['output']\n",
    "\n",
    "x_test = test_df.iloc[:, :-1].values  # Selecting all columns except the last one\n",
    "y_test = test_df.iloc[:, -1].values  # Selecting the last column as y_test\n",
    "\n",
    "# Normalize x_test (divide by 255)\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Apply LabelBinarizer to y_test\n",
    "label_binarizer = LabelBinarizer()\n",
    "y_test = label_binarizer.fit_transform(y_test)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test)\n",
    "y_test_tensor = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba6d6e5",
   "metadata": {},
   "source": [
    "## II. High level\n",
    "\n",
    "### II.1. Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "715ce675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (layer1): Linear(in_features=36, out_features=10, bias=True)\n",
      "  (layer2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (layer3): Linear(in_features=10, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MyModel(pl.LightningModule):\n",
    "    def __init__(self, input_features, num_classes):\n",
    "        super(MyModel, self).__init__()\n",
    "        # Define the layers\n",
    "        self.layer1 = nn.Linear(input_features, 10)  # (input_features, 10)\n",
    "        self.layer2 = nn.Linear(10, 10)  # (10, 10)\n",
    "        self.layer3 = nn.Linear(10, num_classes)  # (10, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass with ReLU activations and Softmax output\n",
    "        x = F.relu(self.layer1(x))  # Apply ReLU activation after the first dense layer\n",
    "        x = F.relu(self.layer2(x))  # Apply ReLU activation after the second dense layer\n",
    "        x = self.layer3(x)  # Output layer (no activation here)\n",
    "        return F.softmax(x, dim=1)  # Apply Softmax activation along dimension 1 (for class probabilities)\n",
    "\n",
    "# Assuming the number of features in your dataset is 5 and the number of classes is 3\n",
    "input_features = x_train_tensor.shape[1]  # Number of features from your dataset (e.g., 5)\n",
    "num_classes = 6  # Example for 3 output classes\n",
    "\n",
    "# Instantiate the model\n",
    "model = MyModel(input_features, num_classes)\n",
    "\n",
    "# Print the model to see its structure\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b8ada7",
   "metadata": {},
   "source": [
    "### II.2. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6434ac36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.7983129024505615\n",
      "Epoch [2/10], Loss: 1.7979542016983032\n",
      "Epoch [3/10], Loss: 1.7976038455963135\n",
      "Epoch [4/10], Loss: 1.7972749471664429\n",
      "Epoch [5/10], Loss: 1.7969673871994019\n",
      "Epoch [6/10], Loss: 1.7966729402542114\n",
      "Epoch [7/10], Loss: 1.79637610912323\n",
      "Epoch [8/10], Loss: 1.7960841655731201\n",
      "Epoch [9/10], Loss: 1.7958147525787354\n",
      "Epoch [10/10], Loss: 1.795547604560852\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define Adam optimizer with a learning rate of 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "\n",
    "x_train_tensor = x_train_tensor.float()\n",
    "num_epochs = 10  # Define the number of epochs (iterations)\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    outputs = model(x_train_tensor)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(outputs, y_train_tensor.argmax(dim=1))  # Convert y_train_tensor to class indices for CrossEntropyLoss\n",
    "    \n",
    "    # Zero the gradients before running the backward pass\n",
    "    optimizer.zero_grad() ####################################!!!!!!! why we used that ??\n",
    "    \n",
    "    # Backward pass: Compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Optimization step: Update the model parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print the loss for this epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c00065f",
   "metadata": {},
   "source": [
    "### II.3. Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06522f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.23      1.00      0.37       461\n",
      "           2       0.00      0.00      0.00       224\n",
      "           3       0.00      0.00      0.00       397\n",
      "           4       0.00      0.00      0.00       211\n",
      "           5       0.00      0.00      0.00       237\n",
      "           7       0.00      0.00      0.00       470\n",
      "\n",
      "    accuracy                           0.23      2000\n",
      "   macro avg       0.04      0.17      0.06      2000\n",
      "weighted avg       0.05      0.23      0.09      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\imed\\Desktop\\SID\\ML\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\imed\\Desktop\\SID\\ML\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\imed\\Desktop\\SID\\ML\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    # Get the raw output (logits) for X_test\n",
    "    outputs = model(x_test_tensor.float())  # Ensure the input is float32\n",
    "    \n",
    "    # Convert the output probabilities to predicted class labels\n",
    "    _, predicted_classes = torch.max(outputs, dim=1)\n",
    "\n",
    "\n",
    "# Convert predicted class indices back to original class labels\n",
    "predicted_labels = label_binarizer.inverse_transform(predicted_classes.reshape(-1,1).cpu().numpy())\n",
    "\n",
    "# Assuming y_test contains the true labels in their original form\n",
    "y_test_labels = label_binarizer.inverse_transform(y_test_tensor.cpu().numpy())\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb730a6",
   "metadata": {},
   "source": [
    "## III. High level with a custom class\n",
    "\n",
    "### III.1. Custom Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3f09d715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, activation='linear'):\n",
    "        super(MyLayer, self).__init__()\n",
    "        \n",
    "        # Ensure that inputs and outputs are greater than 0\n",
    "        assert in_features > 0, \"Number of inputs must be greater than 0\"\n",
    "        assert out_features > 0, \"Number of outputs must be greater than 0\"\n",
    "        \n",
    "        # Define the linear layer\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
    "        \n",
    "        # Set the activation function based on the string provided\n",
    "        if activation == 'relu':\n",
    "            self.activation = F.relu\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = torch.sigmoid\n",
    "        elif activation == 'linear':\n",
    "            self.activation = None  # No activation (linear)\n",
    "        else:\n",
    "            raise ValueError(\"Activation function must be one of ['relu', 'sigmoid', 'linear']\")\n",
    "        \n",
    "        # Store out_features as a class attribute\n",
    "        self._out_features = out_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply the linear transformation\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        # Apply the activation function if any\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    @property\n",
    "    def bias(self):\n",
    "        # Access bias through the Linear layer\n",
    "        return self.linear.bias\n",
    "    \n",
    "    @property\n",
    "    def weight(self):\n",
    "        # Access weight through the Linear layer\n",
    "        return self.linear.weight\n",
    "    \n",
    "    @property\n",
    "    def out_features(self):\n",
    "        # Access the number of output features\n",
    "        return self._out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b1b1f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AssertionError('Number of inputs must be greater than 0')\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "# Must print an 'Exception' or 'AssertionError'\n",
    "\n",
    "try:\n",
    "    ml1 = MyLayer(0, 2)\n",
    "except Exception as e:\n",
    "    print(repr(e))\n",
    "\n",
    "print('end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "19b7fe99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================\n",
      "MyLayer(\n",
      "  (linear): Linear(in_features=3, out_features=2, bias=False)\n",
      ")\n",
      "-------------------------------\n",
      "bias= None\n",
      "output= tensor([[0.3790, 0.0000],\n",
      "        [0.0446, 0.0000]], grad_fn=<ReluBackward0>)\n",
      "===============================\n",
      "MyLayer(\n",
      "  (linear): Linear(in_features=3, out_features=2, bias=True)\n",
      ")\n",
      "-------------------------------\n",
      "bias= Parameter containing:\n",
      "tensor([ 0.5747, -0.0874], requires_grad=True)\n",
      "output= tensor([[0.6050, 0.1856],\n",
      "        [0.3526, 0.0577]], grad_fn=<SigmoidBackward0>)\n",
      "===============================\n",
      "MyLayer(\n",
      "  (linear): Linear(in_features=3, out_features=1, bias=True)\n",
      ")\n",
      "-------------------------------\n",
      "bias= Parameter containing:\n",
      "tensor([-0.3147], requires_grad=True)\n",
      "output= tensor([[-0.4266],\n",
      "        [-0.1021]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "l2ts = [\n",
    "    MyLayer(3, 2, bias=False, activation='relu'),\n",
    "    MyLayer(3, 2, bias=True, activation='sigmoid'),\n",
    "    MyLayer(3, 1)\n",
    "    ]\n",
    "\n",
    "XX = Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "for l in l2ts:\n",
    "    print('===============================')\n",
    "    print(l)\n",
    "    print('-------------------------------')\n",
    "    print('bias=', l.bias)\n",
    "    weight = Tensor(l.weight)\n",
    "    print('output=', l(XX))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd89a6f1",
   "metadata": {},
   "source": [
    "### III.2. Custom Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "adde9856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyMLP, self).__init__()\n",
    "\n",
    "        self.lock = False\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.loss_fn = None\n",
    "        self.optimizer = None\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        if self.lock:\n",
    "            raise Exception(\"Cannot add layer: the model is locked.\")\n",
    "        \n",
    "        if len(self.layers) > 0:\n",
    "            previous_layer_output = self.layers[-1].out_features\n",
    "            current_layer_input = layer.in_features\n",
    "            if previous_layer_output != current_layer_input:\n",
    "                raise Exception(f\"Layer input size ({current_layer_input}) does not match previous layer output size ({previous_layer_output}).\")\n",
    "        \n",
    "        self.layers.append(layer)\n",
    "        return self\n",
    "\n",
    "    def compile(self, num_inputs=1, num_outputs=1, bias=True, multi_class=False, learning_rate=1.0):\n",
    "        if self.lock:\n",
    "            raise Exception(\"Cannot compile: the model is locked.\")\n",
    "        \n",
    "        if len(self.layers) > 0:\n",
    "            num_inputs = self.layers[-1].out_features\n",
    "        \n",
    "        output_layer = MyLayer(num_inputs, num_outputs, bias=bias)\n",
    "        self.add_layer(output_layer)\n",
    "        \n",
    "        if multi_class and num_outputs > 1:\n",
    "            self.activation_fn = nn.Softmax(dim=1)\n",
    "            self.loss_fn = nn.CrossEntropyLoss()\n",
    "        else:\n",
    "            self.activation_fn = nn.Sigmoid()\n",
    "            self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.lock = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.activation_fn(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, X, Y):\n",
    "        predictions = self.forward(X)\n",
    "        loss = self.loss_fn(predictions, Y)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.detach().numpy()\n",
    "\n",
    "    def fit(self, X, Y, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            loss = self.backward(X, Y)\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss}\")\n",
    "\n",
    "    def __call__(self, X, Y):\n",
    "        return self.backward(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84994fdd",
   "metadata": {},
   "source": [
    "### III.3. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3b1ff947",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MyLayer' object has no attribute 'in_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m nn2 \u001b[38;5;241m=\u001b[39m MyMLP()\n\u001b[0;32m      2\u001b[0m \u001b[43mnn2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMyLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m----> 3\u001b[0m \u001b[43m   \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMyLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\\\n\u001b[0;32m      4\u001b[0m    \u001b[38;5;241m.\u001b[39mcompile(nb_out\u001b[38;5;241m=\u001b[39my_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, multiclass\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m nn2\n",
      "Cell \u001b[1;32mIn[56], line 16\u001b[0m, in \u001b[0;36mMyMLP.add_layer\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     15\u001b[0m     previous_layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mout_features\n\u001b[1;32m---> 16\u001b[0m     current_layer_input \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_features\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_layer_output \u001b[38;5;241m!=\u001b[39m current_layer_input:\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_layer_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) does not match previous layer output size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprevious_layer_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\imed\\Desktop\\SID\\ML\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1933\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MyLayer' object has no attribute 'in_features'"
     ]
    }
   ],
   "source": [
    "nn2 = MyMLP()\n",
    "nn2.add_layer(MyLayer(x_train.shape[1], 10, activation='relu'))\\\n",
    "   .add_layer(MyLayer(10, 10, activation='relu'))\\\n",
    "   .compile(nb_out=y_train.shape[1], lr=0.01, multiclass=True)\n",
    "\n",
    "nn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f42cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc07c7fb",
   "metadata": {},
   "source": [
    "### III.4. Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06871b0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ca87fb3",
   "metadata": {},
   "source": [
    "## IV. Low level\n",
    "\n",
    "### IV.1. Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0525616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699879d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = Tensor([[1., -1., 0.], [-0.5, 0.2, 5]])\n",
    "sigmoid = SimpleSigmoid()\n",
    "print(sigmoid(XX))\n",
    "relu = SimpleReLU()\n",
    "print(relu(XX))\n",
    "softmax = SimpleSoftmax()\n",
    "print(softmax(XX))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110e9570",
   "metadata": {},
   "source": [
    "### IV.2. Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3709a590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e02b51e4",
   "metadata": {},
   "source": [
    "### IV.3. Optimization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3645425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a238453d",
   "metadata": {},
   "source": [
    "### IV.4. Custom Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a757d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleLayer in here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6c2644",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = SimpleLayer(3, 2, bias=False)\n",
    "\n",
    "sl.randomize()\n",
    "sl.b, sl.W, list(sl.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4146479",
   "metadata": {},
   "source": [
    "### IV.5. Custom Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d381678b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9faa772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result:\n",
    "# tensor([[0.8401],\n",
    "#         [0.8428]], grad_fn=<MulBackward0>)\n",
    "# 1.0020916\n",
    "# Parameter containing:\n",
    "# tensor([[0.5149],\n",
    "#         [0.5659]], requires_grad=True)\n",
    "\n",
    "nn3t = SimpleMLP()\n",
    "nn3t.add_layer(SimpleLayer(2, 2, act='sigmoid'))\\\n",
    "    .add_layer(SimpleLayer(2, 2, act='sigmoid'))\\\n",
    "    .compile()\n",
    "\n",
    "# print(nn3)\n",
    "\n",
    "with torch.no_grad():\n",
    "    nn3t.layers[0].W += torch.Tensor([[0.5, 0.3], [0.2, 0.4]])\n",
    "    nn3t.layers[0].b += torch.Tensor([[-0.3, 0.5]])\n",
    "    nn3t.layers[1].W += torch.Tensor([[0.3, -0.1], [0.5, -0.3]])\n",
    "    nn3t.layers[1].b += torch.Tensor([[-0.3, -0.2]])\n",
    "    nn3t.layers[2].W  += torch.Tensor([[0.7], [0.7]])\n",
    "    nn3t.layers[2].b  += torch.Tensor([[1.]])\n",
    "\n",
    "XX = Tensor([[2, -1], [3, 5]])\n",
    "YY = Tensor([[0], [1]])\n",
    "\n",
    "print(nn3t.forward(XX))\n",
    "\n",
    "loss = nn3t.backward(XX, YY)\n",
    "\n",
    "print(loss)\n",
    "\n",
    "nn3t.layers[2].W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251617ac",
   "metadata": {},
   "source": [
    "### IV.6. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d472a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6132a19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e255711c",
   "metadata": {},
   "source": [
    "### IV.7. Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c82465",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0326f29d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
